# ğŸ“„ LLM PDF Applicant Info Extractor

<div align="center">

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104.1-green.svg)
![Next.js](https://img.shields.io/badge/Next.js-15.2.4-black.svg)

A modern, full-stack application that intelligently extracts applicant information from PDF documents using OpenAI's GPT-4 and advanced RAG (Retrieval Augmented Generation) techniques.

[Features](#-features) â€¢ [Quick Start](#-quick-start) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [Security](#-security) â€¢ [API Documentation](#-api-documentation)

</div>

---

## âœ¨ Features

- ğŸ¤– **AI-Powered Extraction**: Leverages GPT-4 to intelligently extract applicant information
- ğŸ“Š **Smart Processing**: Automatically chooses between direct processing and RAG based on document size
- ğŸ¨ **Modern UI**: Beautiful, responsive Next.js frontend with Tailwind CSS and shadcn/ui components
- âš¡ **Fast & Efficient**: Optimized token usage with intelligent chunking for large documents
- ğŸ”’ **Security First**: Built with security best practices (see [Security](#-security) section)
- ğŸ“± **Responsive Design**: Works seamlessly on desktop and mobile devices
- ğŸ¯ **Type-Safe**: Full TypeScript support on the frontend
- ğŸ³ **Docker Ready**: Easy deployment with Docker and Docker Compose
- ğŸ“¦ **Modular Architecture**: Clean, maintainable code structure

## ğŸ¯ What It Extracts

The application can extract various applicant information including:

- ğŸ“š GPA (Grade Point Average)
- ğŸ“ Intended Major
- ğŸ“ Test Scores (SAT, ACT, etc.)
- ğŸ‘¤ Personal Information
- And more...

## ğŸš€ Quick Start

### Option 1: Using Scripts (Recommended)

```bash
# 1. Clone the repository
git clone <your-repo-url>
cd LLM-Sumamry-Transcript

# 2. Set up environment variables
cp env.example .env
# Edit .env and add your OPENAI_API_KEY

# 3. Make scripts executable (Unix/Mac)
chmod +x scripts/*.sh

# 4. Start everything
./scripts/start-all.sh
```

### Option 2: Using Docker

```bash
# 1. Clone and configure
git clone <your-repo-url>
cd LLM-Sumamry-Transcript
cp env.example .env
# Edit .env and add your OPENAI_API_KEY

# 2. Start with Docker Compose
docker-compose up
```

### Option 3: Manual Start

```bash
# Terminal 1 - Backend
./scripts/start-backend.sh

# Terminal 2 - Frontend
./scripts/start-frontend.sh
```

**Access the application:**

- Frontend: http://localhost:3000
- Backend API: http://127.0.0.1:8000
- API Documentation: http://127.0.0.1:8000/docs

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Next.js UI    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  FastAPI Backend â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   OpenAI GPT-4  â”‚
â”‚  (TypeScript)   â”‚  HTTP   â”‚     (Python)     â”‚   API   â”‚   + Embeddings  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  FAISS Vector DB â”‚
                            â”‚   (for RAG)      â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Processing Flow

1. **Small Documents** (â‰¤4000 tokens): Direct GPT-4 processing
2. **Large Documents** (>4000 tokens): RAG with FAISS vector store
   - Document is split into chunks
   - Chunks are embedded and stored in FAISS
   - Relevant chunks are retrieved for context
   - GPT-4 processes with retrieved context

## ğŸ“ Project Structure

```
LLM-Sumamry-Transcript/
â”œâ”€â”€ backend/                 # Python FastAPI backend
â”‚   â”œâ”€â”€ api/                # API routes
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ routes.py       # Endpoint definitions
â”‚   â”œâ”€â”€ config/             # Configuration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py     # Environment settings
â”‚   â”œâ”€â”€ services/           # Business logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ pdf_processor.py   # PDF text extraction
â”‚   â”‚   â”œâ”€â”€ llm_service.py     # LLM interactions
â”‚   â”‚   â””â”€â”€ pdf_generator.py   # PDF report generation
â”‚   â”œâ”€â”€ utils/              # Utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ validators.py   # File validation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py             # FastAPI app entry point
â”‚
â”œâ”€â”€ app/                    # Next.js app directory
â”‚   â”œâ”€â”€ page.tsx           # Main upload page
â”‚   â”œâ”€â”€ layout.tsx         # Root layout
â”‚   â””â”€â”€ globals.css        # Global styles
â”‚
â”œâ”€â”€ components/            # React components
â”‚   â”œâ”€â”€ ui/               # shadcn/ui components
â”‚   â””â”€â”€ theme-provider.tsx
â”‚
â”œâ”€â”€ scripts/              # Startup scripts
â”‚   â”œâ”€â”€ start-backend.sh  # Start backend (Unix/Mac)
â”‚   â”œâ”€â”€ start-backend.bat # Start backend (Windows)
â”‚   â”œâ”€â”€ start-frontend.sh # Start frontend
â”‚   â””â”€â”€ start-all.sh      # Start both services
â”‚
â”œâ”€â”€ outputs/              # Generated PDF reports
â”œâ”€â”€ public/               # Static assets
â”œâ”€â”€ lib/                  # Frontend utilities
â”œâ”€â”€ hooks/                # React hooks
â”‚
â”œâ”€â”€ Dockerfile            # Docker configuration
â”œâ”€â”€ docker-compose.yml    # Docker Compose setup
â”œâ”€â”€ .dockerignore         # Docker ignore rules
â”‚
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ package.json          # Node.js dependencies
â”œâ”€â”€ env.example          # Environment variables template
â”œâ”€â”€ .gitignore           # Git ignore rules
â”‚
â”œâ”€â”€ README.md            # This file
â””â”€â”€ SECURITY.md          # Security documentation
```

## ğŸ“¦ Installation

### Prerequisites

- Python 3.8 or higher
- Node.js 18 or higher
- pnpm (or npm/yarn)
- OpenAI API key

### Backend Setup

1. **Clone the repository**

   ```bash
   git clone <your-repo-url>
   cd LLM-Sumamry-Transcript
   ```

2. **Create a virtual environment** (recommended)

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install Python dependencies**

   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables**

   ```bash
   cp env.example .env
   ```

   Edit `.env` and add your OpenAI API key:

   ```env
   OPENAI_API_KEY=your_actual_api_key_here
   ALLOWED_ORIGINS=http://localhost:3000
   ```

5. **Start the FastAPI server**

   ```bash
   # Using the script (recommended)
   ./scripts/start-backend.sh

   # Or manually
   uvicorn backend.main:app --reload
   ```

   The API will be available at `http://127.0.0.1:8000`

### Frontend Setup

1. **Install Node.js dependencies**

   ```bash
   pnpm install
   # or: npm install
   # or: yarn install
   ```

2. **Start the development server**

   ```bash
   # Using the script (recommended)
   ./scripts/start-frontend.sh

   # Or manually
   pnpm dev
   # or: npm run dev
   # or: yarn dev
   ```

   The frontend will be available at `http://localhost:3000`

## ğŸ“– Usage

### Using the Web Interface

1. Open your browser and navigate to `http://localhost:3000`
2. Click the upload area or drag and drop a PDF file
3. Click "Upload Documents" to process
4. Wait for the AI to extract the information
5. View the extracted results on the page

### Using the API Directly

```bash
curl -X POST "http://127.0.0.1:8000/api/v1/process" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@/path/to/your/document.pdf"
```

Response:

```json
{
  "tokens": 1234,
  "mode": "direct",
  "response": "Extracted information here...",
  "output_file": "outputs/output.pdf"
}
```

## ğŸ”’ Security

This project takes security seriously. Here are the security measures implemented:

### âœ… Implemented Security Features

- âœ… **Environment Variables**: API keys stored securely in `.env` (not committed to git)
- âœ… **CORS Protection**: Configurable allowed origins (no wildcard `*` in production)
- âœ… **File Size Limits**: Maximum 10MB file size to prevent DoS attacks
- âœ… **File Type Validation**: Validates actual file content, not just extensions
- âœ… **Input Sanitization**: Proper error handling and validation
- âœ… **Modular Architecture**: Separation of concerns for better security
- âœ… **Secure Dependencies**: Using specific version pins in `requirements.txt`

### ğŸ” Security Best Practices for Production

See [SECURITY.md](SECURITY.md) for detailed security information.

Quick checklist:

- [ ] Set proper CORS origins in `.env`
- [ ] Use HTTPS in production
- [ ] Add rate limiting
- [ ] Implement authentication if needed
- [ ] Set up monitoring and logging
- [ ] Review and limit API permissions
- [ ] Keep dependencies updated

## ğŸ“š API Documentation

### Endpoints

#### `POST /api/v1/process`

Process a PDF file and extract applicant information.

**Request:**

- Method: `POST`
- Content-Type: `multipart/form-data`
- Body: PDF file

**Response:**

```typescript
{
  tokens: number; // Number of tokens in the document
  mode: "direct" | "RAG"; // Processing mode used
  response: string; // Extracted information
  output_file: string; // Path to generated PDF report
}
```

**Status Codes:**

- `200`: Success
- `400`: Invalid file or bad request
- `413`: File too large
- `500`: Server error

#### `GET /api/v1/health`

Health check endpoint.

**Response:**

```json
{
  "status": "healthy",
  "service": "pdf-extractor"
}
```

### Interactive API Docs

When the server is running, visit:

- Swagger UI: `http://127.0.0.1:8000/docs`
- ReDoc: `http://127.0.0.1:8000/redoc`

## ğŸ› ï¸ Technology Stack

### Backend

- **FastAPI**: Modern, fast web framework for building APIs
- **PyMuPDF (fitz)**: PDF text extraction
- **LangChain**: LLM orchestration and RAG implementation
- **FAISS**: Vector similarity search for RAG
- **OpenAI**: GPT-4 and embeddings
- **tiktoken**: Token counting
- **FPDF**: PDF generation
- **python-magic**: File type validation

### Frontend

- **Next.js 15**: React framework with App Router
- **TypeScript**: Type-safe JavaScript
- **Tailwind CSS**: Utility-first CSS framework
- **shadcn/ui**: Beautiful, accessible component library
- **Lucide React**: Icon library

## ğŸ³ Docker Deployment

### Using Docker Compose (Recommended)

```bash
# Start all services
docker-compose up

# Start in detached mode
docker-compose up -d

# Stop services
docker-compose down

# View logs
docker-compose logs -f
```

### Using Docker Directly

```bash
# Build the image
docker build -t pdf-extractor .

# Run the container
docker run -p 8000:8000 \
  -e OPENAI_API_KEY=your_key_here \
  -v $(pwd)/outputs:/app/outputs \
  pdf-extractor
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

### Development Guidelines

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ§ª Testing

```bash
# Backend tests (coming soon)
pytest

# Frontend tests (coming soon)
pnpm test
```

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- OpenAI for GPT-4 and embeddings API
- LangChain for the excellent LLM framework
- FastAPI for the amazing web framework
- shadcn for the beautiful UI components

## ğŸ“§ Support

If you have any questions or run into issues, please:

1. Check the [API Documentation](#-api-documentation)
2. Review the [Security](#-security) section
3. Read [SECURITY.md](SECURITY.md) for detailed security information
4. Open an issue on GitHub

## ğŸ—ºï¸ Roadmap

- [ ] Add support for more document formats (DOCX, TXT)
- [ ] Implement user authentication
- [ ] Add batch processing capabilities
- [ ] Add comprehensive test suite
- [ ] Implement caching for faster repeated queries
- [ ] Add support for custom extraction templates
- [ ] Create admin dashboard for monitoring
- [ ] Add rate limiting middleware
- [ ] Implement request logging
- [ ] Add API key rotation

## ğŸ“Š Performance

- **Small documents** (<4000 tokens): ~2-5 seconds
- **Large documents** (>4000 tokens): ~10-30 seconds (depends on size)
- **Concurrent requests**: Supports multiple simultaneous uploads
- **Token efficiency**: Optimized chunking reduces API costs

## ğŸŒ Environment Variables

All configurable options are available in `env.example`. Copy it to `.env` and customize:

```bash
cp env.example .env
```

Key variables:

- `OPENAI_API_KEY`: Your OpenAI API key (required)
- `ALLOWED_ORIGINS`: CORS allowed origins
- `MAX_FILE_SIZE`: Maximum upload size in bytes
- `MODEL_NAME`: OpenAI model to use (default: gpt-4)
- `MAX_TOKENS`: Token threshold for RAG mode
- `OUTPUT_DIR`: Directory for generated reports

---

<div align="center">

Made with â¤ï¸ using FastAPI and Next.js

**Version 2.0.0** - Refactored with improved architecture and security

</div>
